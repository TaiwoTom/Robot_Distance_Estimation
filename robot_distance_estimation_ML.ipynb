{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlnuucKTZXo1","outputId":"f5e1ab79-c226-498b-d0e8-90c000e08b09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing curve_slow data...\n","       motion  augmentation          model           MAE          RMSE  \\\n","0  curve_slow      original  Random Forest  47826.093672  55414.041889   \n","1  curve_slow      original            SVR  42742.801793  49356.322766   \n","2  curve_slow     jittering  Random Forest  47826.093672  55414.041889   \n","3  curve_slow     jittering            SVR  42742.801793  49356.322766   \n","4  curve_slow       scaling  Random Forest  47826.093672  55414.041889   \n","5  curve_slow       scaling            SVR  42742.801793  49356.322766   \n","6  curve_slow  time_warping  Random Forest  47826.093672  55414.041889   \n","7  curve_slow  time_warping            SVR  42742.801793  49356.322766   \n","\n","         R2  \n","0 -0.260705  \n","1 -0.000136  \n","2 -0.260705  \n","3 -0.000136  \n","4 -0.260705  \n","5 -0.000136  \n","6 -0.260705  \n","7 -0.000136  \n","\n","Best Random Forest Result:\n","motion             curve_slow\n","augmentation         original\n","model           Random Forest\n","MAE              47826.093672\n","RMSE             55414.041889\n","R2                  -0.260705\n","Name: 0, dtype: object\n","\n","Best SVR Result:\n","motion            curve_slow\n","augmentation        original\n","model                    SVR\n","MAE             42742.801793\n","RMSE            49356.322766\n","R2                 -0.000136\n","Name: 1, dtype: object\n"]}],"source":["# -*- coding: utf-8 -*-\n","\n","import pandas as pd\n","import numpy as np\n","from scipy.signal import savgol_filter\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.svm import SVR\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Define a function to normalize data\n","def normalize(df, cols):\n","    return (df[cols] - df[cols].min()) / (df[cols].max() - df[cols].min())\n","\n","# Resampling and interpolation to handle varying frequencies\n","def resample_data(df, target_frequency=30):\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n","    df = df.set_index('timestamp').resample(f'{int(1000 / target_frequency)}ms').mean()\n","    df = df.interpolate(method='linear').reset_index()\n","    return df\n","\n","# Integrate acceleration to obtain velocity\n","def integrate_acceleration_to_velocity(df, sampling_rate):\n","    velocity = np.cumsum(df[['x', 'y', 'z']].values, axis=0) * (1 / sampling_rate)\n","    return pd.DataFrame(velocity, columns=['vx', 'vy', 'vz'])\n","\n","# Integrate velocity to estimate distance\n","def integrate_velocity_to_distance(velocity_df, sampling_rate):\n","    distance = np.cumsum(np.sqrt(velocity_df['vx']**2 + velocity_df['vy']**2 + velocity_df['vz']**2)) * (1 / sampling_rate)\n","    return distance\n","\n","# Compute features for a given segment\n","def compute_features(segment):\n","    features = {\n","        'mean_x': segment['x'].mean(),\n","        'std_x': segment['x'].std(),\n","        'integral_x': np.trapz(segment['x']),\n","        # Repeat for y and z\n","    }\n","    return features\n","\n","# Evaluate models\n","def evaluate_model(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n","    r2 = r2_score(y_true, y_pred)\n","    return mae, rmse, r2\n","\n","# Simulate longer trajectories by concatenating 5-meter segments\n","def create_longer_trajectories(df, target_distance=50):\n","    repeat_count = target_distance // 5\n","    longer_trajectory = pd.concat([df] * repeat_count).reset_index(drop=True)\n","    cumulative_distance = np.cumsum([5] * len(longer_trajectory))\n","    return longer_trajectory, cumulative_distance\n","\n","# Main code for processing datasets and model training\n","motion_types = ['curve_slow'] #, 'curve_fast', 'straight_slow', 'straight_fast']\n","all_results = []\n","\n","# Data Augmentation\n","def augment_data(df, jitter_std=0.01, scaling_factor_range=(0.9, 1.1), time_warp_factor_range=(0.8, 1.2)):\n","    augmented_dfs = []\n","    labels = []\n","    # Original data (unchanged)\n","    augmented_dfs.append(df.copy())\n","\n","\n","    labels.append(\"original\")\n","\n","    # Jittering: Add random noise to simulate sensor variation\n","    jittered_df = df.copy()\n","    jittered_df[['x', 'y', 'z']] += np.random.normal(0, jitter_std, df[['x', 'y', 'z']].shape)\n","    augmented_dfs.append(jittered_df)\n","    labels.append(\"jittering\")\n","\n","    # Scaling: Adjust amplitude of the sensor readings\n","    scaling_factor = np.random.uniform(*scaling_factor_range)\n","    scaled_df = df.copy()\n","    scaled_df[['x', 'y', 'z']] *= scaling_factor\n","    augmented_dfs.append(scaled_df)\n","    labels.append(\"scaling\")\n","\n","    # Time Warping: Stretch or compress the time axis\n","    time_warp_factor = np.random.uniform(*time_warp_factor_range)\n","    warped_df = df.copy()\n","    warped_indices = np.round(np.linspace(0, len(df) - 1, int(len(df) * time_warp_factor))).astype(int)\n","    warped_indices = np.clip(warped_indices, 0, len(df) - 1)\n","    warped_df = df.iloc[warped_indices].reset_index(drop=True)\n","    augmented_dfs.append(warped_df)\n","    labels.append(\"time_warping\")\n","\n","    return augmented_dfs,labels\n","\n","for motion in motion_types:\n","    print(f\"Processing {motion} data...\")\n","\n","    # Load datasets\n","    accelerometer = pd.read_csv(f\"/content/sample_data/{motion}_Accelerometer.csv\")\n","    gyroscope = pd.read_csv(f\"/content/sample_data/{motion}_Gyroscope.csv\")\n","\n","   # print(\"Original Accelerometer Data:\")\n","   # print(accelerometer.head(), \"\\n\")\n","\n","    # Use 'seconds_elapsed' as timestamp\n","    if 'seconds_elapsed' in accelerometer.columns:\n","        accelerometer.rename(columns={'seconds_elapsed': 'timestamp'}, inplace=True)\n","    else:\n","        raise ValueError(\"Expected 'seconds_elapsed' column in Accelerometer data!\")\n","\n","    # Normalize and smooth accelerometer data\n","    accelerometer[['x', 'y', 'z']] = normalize(accelerometer, ['x', 'y', 'z'])\n","\n","    #print(\"Normalized Accelerometer Data:\")\n","    #print(accelerometer[['timestamp', 'x', 'y', 'z']].head(), \"\\n\")\n","\n","    accelerometer_smoothed = savgol_filter(accelerometer[['x', 'y', 'z']], window_length=5, polyorder=2, axis=0)\n","    accelerometer[['x', 'y', 'z']] = accelerometer_smoothed\n","\n","    #print(\"Smoothed Accelerometer Data:\")\n","    #print(accelerometer[['timestamp', 'x', 'y', 'z']].head(), \"\\n\")\n","\n","    # Resample data\n","    accelerometer_resampled = resample_data(accelerometer)\n","\n","   # print(\"Resampled Accelerometer Data:\")\n","   # print(accelerometer_resampled.head(), \"\\n\")\n","\n","    #sampling_rate = 30  # Hz after resampling\n","\n","    # Step 1: Integrate acceleration to obtain velocity\n","    #velocity_df = integrate_acceleration_to_velocity(accelerometer_resampled[['x', 'y', 'z']], sampling_rate)\n","\n","    # Step 2: Integrate velocity to estimate distance\n","    #distance_estimated = integrate_velocity_to_distance(velocity_df, sampling_rate)\n","\n","    # Step 3: Define ground truth distances using 5-meter increments\n","    #ground_truth_distance = np.cumsum([5] * (len(distance_estimated) // (sampling_rate * 5)))  # Reset every 5 meters\n","\n","\n","      # Apply Data Augmentation\n","    #augmented_datasets = augment_data(accelerometer_resampled)\n","    augmented_datasets, augmentation_labels = augment_data(accelerometer_resampled)\n","   # print(f\"Augmented Data for {motion} (Sample Output for Jittering):\")\n","   # print(augmented_datasets[1].head(), \"\\n\")  # Jittered data sample\n","\n","  #  print(f\"Augmented Data for {motion} (Sample Output for Scaling):\")\n","  #  print(augmented_datasets[2].head(), \"\\n\")  # Scaled data sample\n","\n","  #  print(f\"Augmented Data for {motion} (Sample Output for Time Warping):\")\n","   # print(augmented_datasets[3].head(), \"\\n\")  # Time-warped data sample\n","\n","    # Process each augmented dataset\n","    for  augmented_df,augmentation_type in zip(augmented_datasets, augmentation_labels):\n","        # Extract features, estimate distance, and prepare data for training\n","        sampling_rate = 30  # Hz after resampling\n","        velocity_df = integrate_acceleration_to_velocity(augmented_df[['x', 'y', 'z']], sampling_rate)\n","        distance_estimated = integrate_velocity_to_distance(velocity_df, sampling_rate)\n","\n","        # Create cumulative distance target\n","        cumulative_distance = np.cumsum([5] * (len(distance_estimated) // (sampling_rate * 5)))\n","\n","        # Define X (features) and y (target)\n","        X = augmented_df[['x', 'y', 'z']]\n","        y = cumulative_distance\n","\n","        # Split data into train and test sets\n","        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","        # Step 4: Create synthetic longer trajectories\n","        longer_trajectory, cumulative_distance = create_longer_trajectories(accelerometer_resampled, target_distance=50)\n","\n","        # Define X (features) and y (target)\n","        X = longer_trajectory[['x', 'y', 'z']]\n","        y = cumulative_distance\n","\n","        # Split data into train and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","        # Train Random Forest Regressor\n","        rfr = RandomForestRegressor(n_estimators=100, random_state=42)\n","        rfr.fit(X_train, y_train)\n","        y_pred_rfr = rfr.predict(X_test)\n","\n","        # Train Support Vector Regressor\n","        svr = SVR(kernel='rbf', C=10, epsilon=0.1)\n","        svr.fit(X_train, y_train)\n","        y_pred_svr = svr.predict(X_test)\n","\n","        # Evaluate models\n","        rfr_metrics = evaluate_model(y_test, y_pred_rfr)\n","        svr_metrics = evaluate_model(y_test, y_pred_svr)\n","        all_results.append({\n","             \"motion\": motion,\n","             \"augmentation\": augmentation_type,\n","             \"model\": \"Random Forest\",\n","             \"MAE\": rfr_metrics[0],\n","             \"RMSE\": rfr_metrics[1],\n","             \"R2\": rfr_metrics[2]\n","         })\n","        all_results.append({\n","             \"motion\": motion,\n","             \"augmentation\": augmentation_type,\n","             \"model\": \"SVR\",\n","             \"MAE\": svr_metrics[0],\n","             \"RMSE\": svr_metrics[1],\n","             \"R2\": svr_metrics[2]\n","         })\n","\n","# Display results\n","results_df = pd.DataFrame(all_results)\n","print(results_df)\n","\n","# Find the best Random Forest result (based on lowest RMSE)\n","best_rfr = results_df[results_df['model'] == 'Random Forest'].sort_values(by='RMSE', ascending=True).iloc[0]\n","\n","# Find the best SVR result (based on lowest RMSE)\n","best_svr = results_df[results_df['model'] == 'SVR'].sort_values(by='RMSE', ascending=True).iloc[0]\n","\n","# Print the best results\n","print(\"\\nBest Random Forest Result:\")\n","print(best_rfr)\n","\n","print(\"\\nBest SVR Result:\")\n","print(best_svr)\n","\n"]}]}